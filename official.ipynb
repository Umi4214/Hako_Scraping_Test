{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientTimeout\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm xử lý phụ \n",
    "\n",
    "# Hàm xử lý ngày tháng năm\n",
    "def parse_date(date_text):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_text, '%d/%m/%Y')\n",
    "        return date_obj.day, date_obj.month, date_obj.year\n",
    "    except ValueError:\n",
    "        print(f\"Unable to parse date: {date_text}\")\n",
    "        return None, None, None\n",
    "    \n",
    "# Hàm để xử lý giá trị rỗng\n",
    "def process_value(value, is_numeric=False):\n",
    "    if value in [None, 'nan', 'NaN', 'N/A', '']:\n",
    "        return 0 if is_numeric else \"NOT FOUND\"\n",
    "    return value    \n",
    "\n",
    "# Hàm chuyển đổi giá trị thành số\n",
    "def convert_to_number(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        value = ''.join(filter(str.isdigit, value))\n",
    "        return int(value) if value else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đăng nhập\n",
    "def login_hako(username, password):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--start-maximized')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        driver.get('https://ln.hako.vn/login')\n",
    "        \n",
    "        username_field = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"name\"))\n",
    "        )\n",
    "        password_field = driver.find_element(By.ID, \"password\")\n",
    "        \n",
    "        username_field.clear()\n",
    "        username_field.send_keys(username)\n",
    "        password_field.clear()\n",
    "        password_field.send_keys(password)\n",
    "        \n",
    "        input(\"Xử lý captcha, xong thì nhấn Enter sau khi chắc chắn đã đăng nhập thành công...\")\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                lambda driver: driver.current_url == \"https://ln.hako.vn/\"\n",
    "            )\n",
    "            print(\"Đăng nhập thành công!\")\n",
    "            return driver\n",
    "        except:\n",
    "            if \"login\" in driver.current_url:\n",
    "                print(\"Đăng nhập thất bại! Vẫn ở trang login.\")\n",
    "            else:\n",
    "                print(\"Đăng nhập thất bại! URL hiện tại:\", driver.current_url)\n",
    "            driver.quit()\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Có lỗi: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm trích xuất thông tin\n",
    "\n",
    "# Trích xuất ID\n",
    "def extract_id_from_url(url):\n",
    "    patterns = [\n",
    "        r'/truyen/(\\d+)',\n",
    "        r'/sang-tac/(\\d+)-',\n",
    "        r'/convert/(\\d+)-'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Trích xuất thông tin \n",
    "def extract_info_from_html(novel_id, html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        title = soup.title.string.strip() if soup.title else \"NOT FOUND\"\n",
    "        title = re.sub(r'^\\(\\d+\\)\\s*', '', title)  \n",
    "        title = re.sub(r' - Cổng Light Novel - Đọc Light Novel$', '', title) \n",
    "            \n",
    "        # Trích xuất link\n",
    "        canonical_link = soup.find('link', rel='canonical')\n",
    "        canonical_url = canonical_link['href'] if canonical_link else \"NOT FOUND\"\n",
    "        canonical_url = canonical_url.replace('docln.net', 'ln.hako.vn')\n",
    "        url_id = extract_id_from_url(canonical_url)  \n",
    "\n",
    "        # Trích xuất phương thức sáng tác\n",
    "        method = \"NOT FOUND\"\n",
    "        method_span = soup.find('div', class_='series-type')\n",
    "        if method_span:\n",
    "            method_link = method_span.find('span')\n",
    "        if method_link and method_link.string:\n",
    "            method = method_link.string.strip()\n",
    "\n",
    "        # Trích xuất thông tin về thể loại\n",
    "        genres = []\n",
    "        manga = \"Not sure\"\n",
    "        anime = \"Not sure\"\n",
    "        cd = \"Not sure\"\n",
    "        origin = \"vietnamese\" if method == \"Truyện sáng tác\" else \"japanese\"\n",
    "        genre_items = soup.find_all(class_='series-gerne-item')\n",
    "        for item in genre_items:\n",
    "            genre_text = item.get_text(strip=True)\n",
    "            if \"Manga\" in genre_text: manga = \"Yes\"\n",
    "            if \"Anime\" in genre_text: anime = \"Yes\"\n",
    "            if \"CD\" in genre_text: cd = \"Yes\"\n",
    "            if \"Chinese\" in genre_text: origin = \"chinese\"\n",
    "            if \"English\" in genre_text: origin = \"english\"\n",
    "            if \"Korean\" in genre_text: origin = \"korean\"\n",
    "            elif not any(word in genre_text for word in [\"Manga\", \"Anime\", \"CD\", \"Chinese\", \"English\", \"Korean\"]):\n",
    "                genres.append(genre_text)\n",
    "        genres = \", \".join(genres) if genres else \"NOT FOUND\"\n",
    "\n",
    "        # Trích xuất link ảnh\n",
    "        image_link = \"NOT FOUND\"\n",
    "        content_div = soup.find('div', class_='content img-in-ratio')\n",
    "        if content_div and 'style' in content_div.attrs:\n",
    "            style = content_div['style']\n",
    "            match = re.search(r\"url\\('([^']+)'\\)\", style)\n",
    "            if match:\n",
    "                image_link = match.group(1)\n",
    "        \n",
    "        # Trích xuất tác giả\n",
    "        author = \"NOT FOUND\"\n",
    "        author_span = soup.find('span', class_='info-name', string='Tác giả:')\n",
    "        if author_span:\n",
    "            info_value_span = author_span.find_next_sibling('span', class_='info-value')\n",
    "            if info_value_span:\n",
    "                author_link = info_value_span.find('a')\n",
    "            if author_link:\n",
    "                author = author_link.string.strip()\n",
    "    \n",
    "        # Trích xuất họa sĩ\n",
    "        artist = \"NOT FOUND\"\n",
    "        artist_span = soup.find('span', class_='info-name', string='Họa sĩ:')\n",
    "        if artist_span:\n",
    "            info_value_span = artist_span.find_next_sibling('span', class_='info-value')\n",
    "            if info_value_span and info_value_span.string:\n",
    "                artist = info_value_span.string.strip()\n",
    "\n",
    "        # Trích xuất kiểu trình bày\n",
    "        showtype = 'web novel' if method == \"Truyện sáng tác\" or artist.lower() in ['NOT FOUND', 'N/A'] else 'light novel'\n",
    "\n",
    "        # Trích xuất tình trạng\n",
    "        state = \"NOT FOUND\"\n",
    "        state_span = soup.find('span', class_='info-name', string='Tình trạng:')\n",
    "        if state_span:\n",
    "            info_value_span = state_span.find_next_sibling('span', class_='info-value')\n",
    "            if info_value_span:\n",
    "                state_link = info_value_span.find('a')\n",
    "                if state_link:\n",
    "                    state = state_link.string.strip() \n",
    "\n",
    "        # Trích xuất số like\n",
    "        like = 0\n",
    "        like_span = soup.find('span', class_='block feature-value')\n",
    "        if like_span:\n",
    "            like_link = like_span.find_next_sibling('span', class_='block feature-name')\n",
    "            if like_link and like_link.string:\n",
    "                like_text = like_link.string.strip()\n",
    "                try:\n",
    "                    like = float(like_text)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        # Trích xuất số từ\n",
    "        nword = 0\n",
    "        nword_span = soup.find('div', class_='statistic-name', string='Số từ')\n",
    "        if nword_span:\n",
    "            nword_link = nword_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if nword_link:\n",
    "                nword = nword_link.string.strip()\n",
    "\n",
    "        # Trích xuất số lượt đánh giá\n",
    "        rate = 0  \n",
    "        rate_span = soup.find('div', class_='statistic-name', string='Đánh giá')\n",
    "        if rate_span:\n",
    "            rate_link = rate_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if rate_link and rate_link.string:\n",
    "                rate_text = rate_link.string.strip()\n",
    "                try:\n",
    "                    rate = float(rate_text)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        # Trích xuất số lượt xem\n",
    "        view = 0\n",
    "        view_span = soup.find('div', class_='statistic-name', string='Lượt xem')\n",
    "        if view_span:\n",
    "            view_link = view_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if view_link:\n",
    "                view = view_link.string.strip()\n",
    "\n",
    "        # Trích xuất số lượt bình luận\n",
    "        ncom = 0\n",
    "        ncom_span = soup.find('span', class_='comments-count')\n",
    "        if ncom_span:\n",
    "            ncom = ncom_span.string.strip()\n",
    "            ncom = re.sub(r'[()]', '', ncom)\n",
    "\n",
    "        # Trích xuất tên gọi khác\n",
    "        fname = None\n",
    "        fname_span = soup.find('div', class_='fact-value')\n",
    "        if fname_span:\n",
    "            fname_links = fname_span.find_all('div', class_='block pad-bottom-5')\n",
    "            if fname_links:\n",
    "                fname_list = [link.get_text(strip=True) for link in fname_links if link.get_text(strip=True)]\n",
    "                fname = chr(10).join(fname_list) if fname_list else 'None'\n",
    "\n",
    "        # Trích xuất chủ thầu\n",
    "        trans = None\n",
    "        id_o_l = None\n",
    "        id_o = None\n",
    "        trans_span = soup.find('span', class_='series-owner_name')\n",
    "        if trans_span:\n",
    "            trans = trans_span.string.strip()\n",
    "            next_o = trans_span.find_next('a')\n",
    "            if next_o and 'href' in next_o.attrs:\n",
    "                id_o_l = next_o['href']\n",
    "                if not id_o_l.startswith(('https://', 'http://')):\n",
    "                    id_o_l = 'https://ln.hako.vn' + id_o_l\n",
    "                id_o_l = id_o_l.replace('docln.net', 'ln.hako.vn')\n",
    "                id_o = re.search(r'/(\\d+)$', id_o_l)\n",
    "                id_o = id_o.group(1) if id_o else None\n",
    "\n",
    "        # Trích xuất nhóm dịch\n",
    "        team = None\n",
    "        id_t_l = None\n",
    "        id_t = None\n",
    "        team_span = soup.find('div', class_='fantrans-value')\n",
    "        if team_span:\n",
    "            team = team_span.string.strip()\n",
    "            next_t = team_span.find_next('a')\n",
    "            if next_t and 'href' in next_t.attrs:\n",
    "                id_t_l = next_t['href']\n",
    "                if not id_t_l.startswith(('https://', 'http://')):\n",
    "                    id_t_l = 'https://ln.hako.vn' + id_t_l\n",
    "                else:\n",
    "                    id_t_l = id_t_l.replace('docln.net', 'ln.hako.vn')\n",
    "                id_t = re.search(r'/nhom-dich/(\\d+)', id_t_l)\n",
    "                id_t = id_t.group(1) if id_t else None\n",
    "\n",
    "        # Trích xuất những người tham gia (nếu có)\n",
    "        atb = None\n",
    "        id_j_l = None\n",
    "        id_j = None  \n",
    "        atb_span = soup.find('div', class_='series-owner_share')\n",
    "        if atb_span:\n",
    "            atb_links = atb_span.find_all('a', class_='ln_info-name')\n",
    "            if atb_links:\n",
    "                atb_list = [link.get_text(strip=True) for link in atb_links if link.get_text(strip=True)]\n",
    "                atb = \", \".join(atb_list) if atb_list else 'None' + \",\"\n",
    "                id_j_l_list = [link.get('href') for link in atb_links if link.get('href')]\n",
    "                id_j_l_list = ['https://ln.hako.vn' + url if not url.startswith(('https://', 'http://')) else url.replace('docln.net', 'ln.hako.vn') for url in id_j_l_list]\n",
    "                id_j_l = \", \".join(id_j_l_list) if id_j_l_list else 'None' + \",\"\n",
    "                id_j_list = [re.search(r'/(\\d+)$', url).group(1) for url in id_j_l_list if re.search(r'/(\\d+)$', url)]\n",
    "                id_j = \", \".join(id_j_list) if id_j_list else 'None' + \",\"\n",
    "\n",
    "        # Trích xuất số tập\n",
    "        def count_vol(soup):\n",
    "            vol_spans = soup.find_all('span', class_='list_vol-title')\n",
    "            return len(vol_spans)\n",
    "        nvol = count_vol(soup)\n",
    "\n",
    "        # Trích xuất số chương\n",
    "        def count_chap(soup):\n",
    "            chap_spans = soup.find_all('div', class_='chapter-name')\n",
    "            return len(chap_spans)\n",
    "        nchap = count_chap(soup)\n",
    "\n",
    "        # Xác định hình thức dựa trên số tập và trạng thái\n",
    "        format_type = \"series\" if nvol > 1 else (\n",
    "            \"Not sure\" if nvol == 1 and state.lower() == \"đang tiến hành\" else \"oneshot\"\n",
    "        )\n",
    "\n",
    "        # Trích xuất thời gian bắt đầu \n",
    "        first_day, first_month, first_year = None, None, None\n",
    "        earliest_date = None  # Chuyển khai báo ra ngoài\n",
    "        chapter_time_divs = soup.find_all('div', class_='chapter-time')\n",
    "        for time_div in chapter_time_divs:\n",
    "            date_text = time_div.get_text(strip=True)\n",
    "            try:\n",
    "                current_date = datetime.strptime(date_text, '%d/%m/%Y')\n",
    "                if earliest_date is None or current_date < earliest_date:\n",
    "                    earliest_date = current_date\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if earliest_date:\n",
    "            first_day = earliest_date.day\n",
    "            first_month = earliest_date.month\n",
    "            first_year = earliest_date.year\n",
    "            \n",
    "        # Trích xuất thời gian cập nhật mới nhất\n",
    "        latest_day, latest_month, latest_year = None, None, None\n",
    "        latest_span = soup.find('div', class_='statistic-name', string='Lần cuối')\n",
    "        if latest_span:\n",
    "            time_div = latest_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if time_div:\n",
    "                time_tag = time_div.find('time', class_='timeago')\n",
    "                if time_tag and 'title' in time_tag.attrs:\n",
    "                    latest_date_text = time_tag['title'].split()[0]  \n",
    "                    latest_day, latest_month, latest_year = parse_date(latest_date_text)\n",
    "\n",
    "        return (url_id,title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "                trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, format_type,\n",
    "                first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting info for novel {novel_id}: {str(e)}\")\n",
    "        return None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý thông tin\n",
    "def process_novel_info(info):\n",
    "    if info is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        (url_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "         author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "         trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, format_type,\n",
    "         first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom) = info\n",
    "        \n",
    "        data = {\n",
    "            'ID': process_value(url_id),\n",
    "            'Tựa đề': process_value(title),\n",
    "            'Link hako': process_value(canonical_url),\n",
    "            'Phương thức sáng tác': process_value(method),\n",
    "            'Thể loại': process_value(genres),\n",
    "            'Manga': process_value(manga),\n",
    "            'Anime': process_value(anime),\n",
    "            'CD': process_value(cd),\n",
    "            'Ngôn ngữ gốc': process_value(origin),\n",
    "            'Link ảnh': process_value(image_link),\n",
    "            'Tác giả': process_value(author),\n",
    "            'Họa sĩ': process_value(artist),\n",
    "            'Loại hình': process_value(showtype),\n",
    "            'Tình trạng': process_value(state),\n",
    "            'Số like': convert_to_number(process_value(like, is_numeric=True)),\n",
    "            'Số từ': convert_to_number(process_value(nword, is_numeric=True)),\n",
    "            'Số lượt đánh giá': convert_to_number(process_value(rate, is_numeric=True)),\n",
    "            'Số lượt xem': convert_to_number(process_value(view, is_numeric=True)),\n",
    "            'Số lượt bình luận': convert_to_number(process_value(ncom, is_numeric=True)),\n",
    "            'Fname': process_value(fname),\n",
    "            'ID chủ thầu': process_value(id_o),\n",
    "            'Chủ thầu': process_value(trans),\n",
    "            'Link chủ thầu': process_value(id_o_l),\n",
    "            'ID nhóm thầu': process_value(id_t),\n",
    "            'Nhóm thầu': process_value(team),\n",
    "            'Link nhóm thầu': process_value(id_t_l),\n",
    "            'ID người tham gia': process_value(id_j),\n",
    "            'Người tham gia': process_value(atb),\n",
    "            'Link người tham gia': process_value(id_j_l),\n",
    "            'Số tập': process_value(nvol),\n",
    "            'Số chương': process_value(nchap),\n",
    "            'Hình thức': process_value(format_type),\n",
    "            'Ngày bắt đầu': process_value(first_day),\n",
    "            'Tháng bắt đầu': process_value(first_month),   \n",
    "            'Năm bắt đầu': process_value(first_year),\n",
    "            'Ngày cập nhật cuối': process_value(latest_day),\n",
    "            'Tháng cập nhật cuối': process_value(latest_month),\n",
    "            'Năm cập nhật cuối': process_value(latest_year)\n",
    "        }\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý thông tin: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lấy ID cuối cùng đã xử lý\n",
    "def get_last_processed_id(filename='hako_data.csv'):\n",
    "    \"\"\"Lấy ID cuối cùng đã xử lý từ csv\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        if not df.empty:\n",
    "            return df['ID'].max()\n",
    "        return 0\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đọc thông tin\n",
    "def get_existing_data(filename='hako_data.csv'):\n",
    "    \"\"\"Đọc dữ liệu từ csv (nếu có):\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep=';', quoting=csv.QUOTE_ALL)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tạo tên cột (file csv trống)\n",
    "def get_column_names():\n",
    "    \"\"\"Trả về danh sách tên cột chuẩn\"\"\"\n",
    "    return ['ID', 'Tựa đề', 'Link hako', 'Phương thức sáng tác', 'Thể loại', \n",
    "            'Manga', 'Anime', 'CD', 'Ngôn ngữ gốc', 'Link ảnh', 'Tác giả', \n",
    "            'Họa sĩ', 'Loại hình', 'Tình trạng', 'Số like', 'Số từ', \n",
    "            'Số lượt đánh giá', 'Số lượt xem', 'Số lượt bình luận', 'Fname',\n",
    "            'ID chủ thầu', 'Chủ thầu', 'Link chủ thầu', 'ID nhóm thầu', \n",
    "            'Nhóm thầu', 'Link nhóm thầu', 'ID người tham gia', 'Người tham gia',\n",
    "            'Link người tham gia', 'Số tập', 'Số chương', 'Hình thức',\n",
    "            'Ngày bắt đầu', 'Tháng bắt đầu', 'Năm bắt đầu',\n",
    "            'Ngày cập nhật cuối', 'Tháng cập nhật cuối', 'Năm cập nhật cuối']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lưu thông tin vào csv\n",
    "def save_to_csv(new_data, filename='hako_data.csv', check_summary=False):\n",
    "    \"\"\"Lưu dữ liệu vào csv\"\"\"\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    existing_df = get_existing_data(filename)\n",
    "    \n",
    "    if existing_df.empty:\n",
    "        columns = get_column_names()\n",
    "        new_df = new_df.reindex(columns=columns)\n",
    "        new_df.to_csv(filename, index=False, sep=';', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "        if check_summary:\n",
    "            print(f\"Đã tạo file mới và lưu {len(new_df)} bản ghi\")\n",
    "        return\n",
    "\n",
    "    existing_df['ID'] = pd.to_numeric(existing_df['ID'])\n",
    "    new_df['ID'] = pd.to_numeric(new_df['ID'])\n",
    "    \n",
    "    duplicate_ids = set(existing_df['ID']).intersection(set(new_df['ID']))\n",
    "    \n",
    "    if duplicate_ids and check_summary:\n",
    "        print(f\"Đã tìm thấy {len(duplicate_ids)} ID trùng lặp, sẽ sớm cập nhật\")\n",
    "    \n",
    "    if duplicate_ids:\n",
    "        existing_df = existing_df[~existing_df['ID'].isin(duplicate_ids)]\n",
    "\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    \n",
    "    combined_df = combined_df.sort_values('ID').reset_index(drop=True)\n",
    "    \n",
    "    combined_df.to_csv(filename, index=False, sep=';', quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    if check_summary:\n",
    "        print(f\"Đã lưu tổng cộng {len(combined_df)} bản ghi\")\n",
    "        print(f\"- Giữ nguyên: {len(existing_df)} bản ghi\")\n",
    "        print(f\"- Cập nhật/Thêm mới: {len(new_df)} bản ghi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tìm ID còn thiếu\n",
    "def get_missing_ids(start_id, end_id, filename='hako_data.csv'):\n",
    "    \"\"\"Tìm các ID còn thiếu trong khoảng: \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep=';', quoting=csv.QUOTE_ALL)\n",
    "        existing_ids = set(df['ID'].astype(int))\n",
    "        all_ids = set(range(start_id, end_id + 1))\n",
    "        missing_ids = all_ids - existing_ids\n",
    "        return sorted(list(missing_ids))\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        return list(range(start_id, end_id + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lưu mấy cái ID không lấy được thông tin (có gì còn chạy lại thử)\n",
    "def save_failed_ids(failed_ids, filename='hako_null.csv'):\n",
    "   \"\"\"Lưu các ID không lấy được vào csv\"\"\"\n",
    "   try:\n",
    "       try:\n",
    "           with open(filename, 'r') as f:\n",
    "               existing_content = f.read().strip()\n",
    "               existing_content = existing_content.replace(',\\n', '\\n')\n",
    "               existing_ids = [int(id) for id in existing_content.replace('\\n', ',').split(',') if id.strip()]\n",
    "       except FileNotFoundError:\n",
    "           existing_ids = []\n",
    "       \n",
    "       all_ids = sorted(set(existing_ids + failed_ids))\n",
    "\n",
    "       with open(filename, 'w') as f:\n",
    "           for i in range(0, len(all_ids), 25):\n",
    "               chunk = all_ids[i:i+25]\n",
    "               f.write(','.join(map(str, chunk)) + ',\\n')\n",
    "               \n",
    "   except Exception as e:\n",
    "       print(f\"Lỗi khi lưu ID thất bại: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để fetch thông tin truyện\n",
    "async def fetch_novel_info(session, id, semaphore, delay=0.5):\n",
    "    \"\"\"Hàm async để fetch thông tin truyện\"\"\"\n",
    "    async with semaphore:  \n",
    "        try:\n",
    "            await asyncio.sleep(delay)  \n",
    "            url = f'https://ln.hako.vn/truyen/{id}-abcd'\n",
    "            async with session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    html_content = await response.text()\n",
    "                    return id, html_content\n",
    "                return id, None\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi fetch ID {id}: {str(e)}\")\n",
    "            return id, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý các ID theo batch\n",
    "async def process_batch_async(ids):\n",
    "    \"\"\"Xử lý một batch các ID bằng async\"\"\"\n",
    "    novel_data = []\n",
    "    failed_ids = []\n",
    "    success_count = 0\n",
    "    \n",
    "    # Cấu hình session\n",
    "    timeout = ClientTimeout(total=30)\n",
    "    conn = aiohttp.TCPConnector(limit=None)\n",
    "    semaphore = asyncio.Semaphore(7)  \n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=conn, timeout=timeout) as session:\n",
    "        # Tạo list các coroutines\n",
    "        tasks = [fetch_novel_info(session, id, semaphore) for id in ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Xử lý kết quả\n",
    "        for id, html_content in results:\n",
    "            if html_content:\n",
    "                try:\n",
    "                    novel_info = extract_info_from_html(id, html_content)\n",
    "                    if novel_info:\n",
    "                        processed_info = process_novel_info(novel_info)\n",
    "                        if processed_info:\n",
    "                            novel_data.append(processed_info)\n",
    "                            print(f\"Đã lấy thông tin truyện ID: {id}\")\n",
    "                            success_count += 1\n",
    "                            continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Lỗi xử lý dữ liệu ID {id}: {str(e)}\")\n",
    "            \n",
    "            failed_ids.append(id)\n",
    "    \n",
    "    return novel_data, failed_ids, success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm check driver \n",
    "def is_driver_alive(driver):\n",
    "   \"\"\"Kiểm tra trạng thái hoạt động của driver\"\"\"\n",
    "   try:\n",
    "       driver.title\n",
    "       return True\n",
    "   except:\n",
    "       return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lấy thông tin truyện\n",
    "def scrape_ids_and_info(driver, target_ids=None, start_id=None, end_id=None, filename='hako_data.csv'):\n",
    "   novel_data = []\n",
    "   failed_ids = []\n",
    "   success_count = 0\n",
    "   batch_size = 5\n",
    "   delay_between_requests = 3\n",
    "   delay_between_batches = 3\n",
    "   \n",
    "   if target_ids:\n",
    "       ids_to_check = target_ids\n",
    "       print(f\"Kiểm tra {len(ids_to_check)} ID được chỉ định\")\n",
    "   else:\n",
    "       missing_ids = get_missing_ids(start_id, end_id, filename)\n",
    "       if not missing_ids:\n",
    "           print(f\"Không có ID nào thiếu trong khoảng từ {start_id} đến {end_id}\")\n",
    "           return []\n",
    "       ids_to_check = missing_ids\n",
    "       print(f\"Tìm thấy {len(ids_to_check)} ID cần xử lý trong khoảng từ {start_id} đến {end_id}\")\n",
    "   \n",
    "   try:\n",
    "       for i in range(0, len(ids_to_check), batch_size):\n",
    "           if not is_driver_alive(driver):\n",
    "               print(\"\\nChrome đã đóng, code đang dừng chạy...\")\n",
    "               raise Exception(\"Chrome đã đóng\")\n",
    "               \n",
    "           batch_ids = ids_to_check[i:i + batch_size]\n",
    "           batch_data = []\n",
    "           \n",
    "           print(f\"\\nXử lý batch {i//batch_size + 1}/{(len(ids_to_check) + batch_size - 1)//batch_size}\")\n",
    "           \n",
    "           for id in batch_ids:\n",
    "               try:\n",
    "                   if not is_driver_alive(driver):\n",
    "                       print(\"\\nChrome đã đóng, code đang dừng chạy...\")\n",
    "                       raise Exception(\"Chrome đã đóng\")\n",
    "                       \n",
    "                   url = f'https://ln.hako.vn/truyen/{id}-abcd'\n",
    "                   driver.get(url)\n",
    "                   time.sleep(delay_between_requests)\n",
    "                   \n",
    "                   if driver.current_url == url:\n",
    "                       failed_ids.append(id)\n",
    "                       continue\n",
    "                       \n",
    "                   html_content = driver.page_source\n",
    "                   novel_info = extract_info_from_html(id, html_content)\n",
    "                   \n",
    "                   if novel_info:\n",
    "                       processed_info = process_novel_info(novel_info)\n",
    "                       if processed_info:\n",
    "                           batch_data.append(processed_info)\n",
    "                           print(f\"Đã lấy thông tin truyện ID: {id}\")\n",
    "                           success_count += 1\n",
    "                   else:\n",
    "                       failed_ids.append(id)\n",
    "               \n",
    "               except Exception as e:\n",
    "                   if \"Chrome đã đóng\" in str(e):\n",
    "                       raise  \n",
    "                   print(f\"Lỗi khi xử lý ID {id}: {str(e)}\")\n",
    "                   failed_ids.append(id)\n",
    "                   continue\n",
    "           \n",
    "           # Lưu dữ liệu của batch\n",
    "           if batch_data:\n",
    "               try:\n",
    "                   save_to_csv(batch_data, filename, show_summary=False)\n",
    "                   novel_data.extend(batch_data)\n",
    "               except Exception as e:\n",
    "                   print(f\"Lỗi khi lưu batch data: {str(e)}\")\n",
    "                   # Thêm các ID trong batch vào failed_ids nếu lưu thất bại\n",
    "                   failed_ids.extend([item['ID'] for item in batch_data])\n",
    "           \n",
    "           # Delay giữa các batch\n",
    "           if i + batch_size < len(ids_to_check):\n",
    "               print(f\"Nghỉ {delay_between_batches} giây trước batch tiếp theo...\")\n",
    "               time.sleep(delay_between_batches)\n",
    "               \n",
    "   except KeyboardInterrupt:\n",
    "       print(\"\\nPhát hiện lệnh dừng từ người dùng. Đang lưu trạng thái hiện tại...\")\n",
    "       \n",
    "   except Exception as e:\n",
    "       if \"Chrome đã đóng\" in str(e):\n",
    "           print(\"\\nCode dừng chạy do Chrome đã đóng.\")\n",
    "       else:\n",
    "           print(f\"\\nLỗi trong quá trình crawl: {str(e)}\")\n",
    "       \n",
    "   finally:\n",
    "       # Lưu ID thất bại\n",
    "       if failed_ids:\n",
    "           try:\n",
    "               save_failed_ids(failed_ids)\n",
    "           except Exception as e:\n",
    "               print(f\"Lỗi khi lưu failed IDs: {str(e)}\")\n",
    "       \n",
    "       # Hiển thị thông báo tổng kết\n",
    "       print(f\"\\nKết quả cuối cùng:\")\n",
    "       print(f\"Tổng số bản ghi thành công: {success_count}\")\n",
    "       print(f\"Tổng số bản ghi thất bại: {len(failed_ids)}\")\n",
    "       if failed_ids:\n",
    "           print(f\"Các ID thất bại đã được lưu vào file hako_null.csv\")\n",
    "   \n",
    "   return novel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đăng nhập thành công!\n",
      "Kiểm tra 1 ID được chỉ định\n",
      "\n",
      "Xử lý batch 1/1\n",
      "Đã lấy thông tin truyện ID: 1\n",
      "\n",
      "Kết quả cuối cùng:\n",
      "Tổng số bản ghi thành công: 1\n",
      "Tổng số bản ghi thất bại: 0\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "   username = \"********\"    # Điền username\n",
    "   password = \"********\"    # Điền password\n",
    "   \n",
    "   while True:  \n",
    "       try:\n",
    "           driver = login_hako(username, password)\n",
    "           \n",
    "           if driver:\n",
    "               choice = input(\"Chọn cách lấy ID (1: Nhập khoảng ID (từ... đến...), 2: Nhập list ID): \")\n",
    "               \n",
    "               if choice == \"1\":\n",
    "                   start_id = int(input(\"Nhập ID bắt đầu: \"))\n",
    "                   end_id = int(input(\"Nhập ID kết thúc: \"))\n",
    "                   novel_data = scrape_ids_and_info(driver, start_id=start_id, end_id=end_id)\n",
    "               else:\n",
    "                   \n",
    "                   input_ids = input(\"Nhập các ID (cách nhau bằng dấu phẩy): \").strip()\n",
    "                   if input_ids:\n",
    "                       target_ids = []\n",
    "                       for id_str in input_ids.split(','):\n",
    "                           id_str = id_str.strip()\n",
    "                           if id_str.isdigit():  \n",
    "                               target_ids.append(int(id_str))\n",
    "                       \n",
    "                       if target_ids:\n",
    "                           novel_data = scrape_ids_and_info(driver, target_ids=target_ids)\n",
    "                       else:\n",
    "                           print(\"Không có ID hợp lệ\")\n",
    "                           continue\n",
    "                   else:\n",
    "                       print(\"Nhập ít nhất một ID:\")\n",
    "                       continue\n",
    "               \n",
    "               driver.quit()\n",
    "               break  \n",
    "           \n",
    "       except Exception as e:\n",
    "           print(f\"Lỗi: {str(e)}\")\n",
    "           retry = input(\"Bạn có muốn thử lại không? (y/n): \")\n",
    "           if retry.lower() != 'y':\n",
    "               break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
