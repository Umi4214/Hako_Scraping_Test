{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm xử lý phụ \n",
    "\n",
    "# Hàm xử lý ngày tháng năm\n",
    "def parse_date(date_text):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_text, '%d/%m/%Y')\n",
    "        return date_obj.day, date_obj.month, date_obj.year\n",
    "    except ValueError:\n",
    "        print(f\"Unable to parse date: {date_text}\")\n",
    "        return None, None, None\n",
    "    \n",
    "# Hàm để xử lý giá trị rỗng\n",
    "def process_value(value, is_numeric=False):\n",
    "    if value in [None, 'nan', 'NaN', 'N/A', '']:\n",
    "        return 0 if is_numeric else \"NOT FOUND\"\n",
    "    return value    \n",
    "\n",
    "# Hàm chuyển đổi giá trị thành số\n",
    "def convert_to_number(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        value = ''.join(filter(str.isdigit, value))\n",
    "        return int(value) if value else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_novel_html(novel_id):\n",
    "    url = f\"https://ln.hako.vn/truyen/{novel_id}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching novel {novel_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm trích xuất thông tin\n",
    "\n",
    "# Trích xuất ID\n",
    "def extract_id_from_url(url):\n",
    "    patterns = [\n",
    "        r'/truyen/(\\d+)',\n",
    "        r'/sang-tac/(\\d+)-',\n",
    "        r'/convert/(\\d+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Trích xuất thông tin \n",
    "def extract_info_from_html(novel_id, html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        title = soup.title.string.strip() if soup.title else \"NOT FOUND\"\n",
    "        title = re.sub(r' - Cổng Light Novel - Đọc Light Novel$', '', title) \n",
    "            \n",
    "        # Trích xuất link\n",
    "        canonical_link = soup.find('link', rel='canonical')\n",
    "        canonical_url = canonical_link['href'] if canonical_link else \"NOT FOUND\"\n",
    "        canonical_url = canonical_url.replace('docln.net', 'ln.hako.vn')\n",
    "        url_id = extract_id_from_url(canonical_url)  # Trích xuất ID từ URL\n",
    "\n",
    "        # Trích xuất phương thức sáng tác\n",
    "        method = \"NOT FOUND\"\n",
    "        method_span = soup.find('div', class_='series-type')\n",
    "        if method_span:\n",
    "            method_link = method_span.find('span')\n",
    "        if method_link and method_link.string:\n",
    "            method = method_link.string.strip()\n",
    "\n",
    "        # Trích xuất thông tin về thể loại\n",
    "        genres = []\n",
    "        manga = \"Not sure\"\n",
    "        anime = \"Not sure\"\n",
    "        cd = \"Not sure\"\n",
    "        origin = \"vietnamese\" if method == \"Truyện sáng tác\" else \"japanese\"\n",
    "        genre_items = soup.find_all(class_='series-gerne-item')\n",
    "        for item in genre_items:\n",
    "            genre_text = item.get_text(strip=True)\n",
    "            if \"Manga\" in genre_text: manga = \"Yes\"\n",
    "            if \"Anime\" in genre_text: anime = \"Yes\"\n",
    "            if \"CD\" in genre_text: cd = \"Yes\"\n",
    "            if \"Chinese\" in genre_text: origin = \"chinese\"\n",
    "            if \"English\" in genre_text: origin = \"english\"\n",
    "            if \"Korean\" in genre_text: origin = \"korean\"\n",
    "            elif not any(word in genre_text for word in [\"Manga\", \"Anime\", \"CD\", \"Chinese\", \"English\", \"Korean\"]):\n",
    "                genres.append(genre_text)\n",
    "\n",
    "        # Trích xuất link ảnh\n",
    "        image_link = \"NOT FOUND\"\n",
    "        content_div = soup.find('div', class_='content img-in-ratio')\n",
    "        if content_div and 'style' in content_div.attrs:\n",
    "            style = content_div['style']\n",
    "            match = re.search(r\"url\\('([^']+)'\\)\", style)\n",
    "            if match:\n",
    "                image_link = match.group(1)\n",
    "        \n",
    "        # Trích xuất tác giả\n",
    "        author = \"NOT FOUND\"\n",
    "        author_span = soup.find('span', class_='info-name', string='Tác giả:')\n",
    "        if author_span:\n",
    "            info_value_span = author_span.find_next_sibling('span', class_='info-value')\n",
    "            if info_value_span:\n",
    "                author_link = info_value_span.find('a')\n",
    "            if author_link:\n",
    "                author = author_link.string.strip()\n",
    "    \n",
    "        # Trích xuất họa sĩ\n",
    "        artist = \"NOT FOUND\"\n",
    "        artist_span = soup.find('span', class_='info-name', string='Họa sĩ:')\n",
    "        if artist_span:\n",
    "            info_value_span = artist_span.find_next_sibling('span', class_='info-value')\n",
    "            if info_value_span and info_value_span.string:\n",
    "                artist = info_value_span.string.strip()\n",
    "\n",
    "        # Trích xuất kiểu trình bày\n",
    "        showtype = 'web novel' if method == \"Truyện sáng tác\" or artist.lower() in ['NOT FOUND', 'N/A'] else 'light novel'\n",
    "\n",
    "        # Trích xuất tình trạng\n",
    "        state = \"NOT FOUND\"\n",
    "        state_span = soup.find('span', class_='info-name', string='Tình trạng:')\n",
    "        if state_span:\n",
    "            info_value_span = state_span.find_next_sibling('span', class_='info-value')\n",
    "            if info_value_span:\n",
    "                state_link = info_value_span.find('a')\n",
    "                if state_link:\n",
    "                    state = state_link.string.strip() \n",
    "\n",
    "        # Trích xuất số like\n",
    "        like = 0\n",
    "        like_span = soup.find('span', class_='block feature-value')\n",
    "        if like_span:\n",
    "            like_link = like_span.find_next_sibling('span', class_='block feature-name')\n",
    "            if like_link and like_link.string:\n",
    "                like_text = like_link.string.strip()\n",
    "                try:\n",
    "                    like = float(like_text)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        # Trích xuất số từ\n",
    "        nword = 0\n",
    "        nword_span = soup.find('div', class_='statistic-name', string='Số từ')\n",
    "        if nword_span:\n",
    "            nword_link = nword_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if nword_link:\n",
    "                nword = nword_link.string.strip()\n",
    "\n",
    "        # Trích xuất số lượt đánh giá\n",
    "        rate = 0  \n",
    "        rate_span = soup.find('div', class_='statistic-name', string='Đánh giá')\n",
    "        if rate_span:\n",
    "            rate_link = rate_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if rate_link and rate_link.string:\n",
    "                rate_text = rate_link.string.strip()\n",
    "                try:\n",
    "                    rate = float(rate_text)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        # Trích xuất số lượt xem\n",
    "        view = 0\n",
    "        view_span = soup.find('div', class_='statistic-name', string='Lượt xem')\n",
    "        if view_span:\n",
    "            view_link = view_span.find_next_sibling('div', class_='statistic-value')\n",
    "            if view_link:\n",
    "                view = view_link.string.strip()\n",
    "\n",
    "        # Trích xuất số lượt bình luận\n",
    "        ncom = 0\n",
    "        ncom_span = soup.find('span', class_='comments-count')\n",
    "        if ncom_span:\n",
    "            ncom = ncom_span.string.strip()\n",
    "            ncom = re.sub(r'[()]', '', ncom)\n",
    "\n",
    "        # Trích xuất tên gọi khác\n",
    "        fname = None\n",
    "        fname_span = soup.find('div', class_='fact-value')\n",
    "        if fname_span:\n",
    "            fname_links = fname_span.find_all('div', class_='block pad-bottom-5')\n",
    "            if fname_links:\n",
    "                fname_list = [link.get_text(strip=True) for link in fname_links if link.get_text(strip=True)]\n",
    "                fname = chr(10).join(fname_list) if fname_list else 'None'\n",
    "\n",
    "        # Trích xuất người dịch\n",
    "        trans = None\n",
    "        id_o_l = None\n",
    "        id_o = None\n",
    "        trans_span = soup.find('span', class_='series-owner_name')\n",
    "        if trans_span:\n",
    "            trans = trans_span.string.strip()\n",
    "            next_o = trans_span.find_next('a')\n",
    "            if next_o and 'href' in next_o.attrs:\n",
    "                id_o_l = next_o['href']\n",
    "                if not id_o_l.startswith(('https://', 'http://')):\n",
    "                    id_o_l = 'https://ln.hako.vn' + id_o_l\n",
    "                else:\n",
    "                    id_o_l = id_o_l.replace('docln.net', 'ln.hako.vn')\n",
    "                    id_o = re.search(r'/(\\d+)$', id_o_l)\n",
    "                    id_o = id_o.group(1) if id_o else None\n",
    "\n",
    "        # Trích xuất nhóm dịch\n",
    "        team = None\n",
    "        id_t_l = None\n",
    "        id_t = None\n",
    "        team_span = soup.find('div', class_='fantrans-value')\n",
    "        if team_span:\n",
    "            team = team_span.string.strip()\n",
    "            next_t = team_span.find_next('a')\n",
    "            if next_t and 'href' in next_t.attrs:\n",
    "                id_t_l = next_t['href']\n",
    "                if not id_t_l.startswith(('https://', 'http://')):\n",
    "                    id_t_l = 'https://ln.hako.vn' + id_t_l\n",
    "                else:\n",
    "                    id_t_l = id_t_l.replace('docln.net', 'ln.hako.vn')\n",
    "                id_t = re.search(r'/nhom-dich/(\\d+)', id_t_l)\n",
    "                id_t = id_t.group(1) if id_t else None\n",
    "\n",
    "        # Trích xuất người tham gia\n",
    "        atb = None\n",
    "        id_j_l = None\n",
    "        id_j = None  \n",
    "        atb_span = soup.find('div', class_='series-owner_share')\n",
    "        if atb_span:\n",
    "            atb_links = atb_span.find_all('a', class_='ln_info-name')\n",
    "            if atb_links:\n",
    "                atb_list = [link.get_text(strip=True) for link in atb_links if link.get_text(strip=True)]\n",
    "                atb = chr(10).join(atb_list) if atb_list else 'None'\n",
    "                id_j_l_list = [link.get('href') for link in atb_links if link.get('href')]\n",
    "                id_j_l_list = ['https://ln.hako.vn' + url if not url.startswith(('https://', 'http://')) else url.replace('docln.net', 'ln.hako.vn') for url in id_j_l_list]\n",
    "                id_j_l = chr(10).join(id_j_l_list) if id_j_l_list else 'None'\n",
    "                id_j_list = [re.search(r'/(\\d+)$', url).group(1) for url in id_j_l_list if re.search(r'/(\\d+)$', url)]\n",
    "                id_j = chr(10).join(id_j_list) if id_j_list else 'None'\n",
    "\n",
    "        # Trích xuất số tập\n",
    "        def count_vol(soup):\n",
    "            vol_spans = soup.find_all('span', class_='list_vol-title')\n",
    "            return len(vol_spans)\n",
    "        nvol = count_vol(soup)\n",
    "\n",
    "        # Trích xuất số chương\n",
    "        def count_chap(soup):\n",
    "            chap_spans = soup.find_all('div', class_='chapter-name')\n",
    "            return len(chap_spans)\n",
    "        nchap = count_chap(soup)\n",
    "\n",
    "        # Xác định hình thức dựa trên số tập và trạng thái\n",
    "        format_type = \"series\" if nvol > 1 else (\n",
    "            \"Not sure\" if nvol == 1 and state.lower() == \"đang tiến hành\" else \"oneshot\"\n",
    "        )\n",
    "\n",
    "        # Trích xuất thời gian bắt đầu và thời gian cập nhật mới nhất\n",
    "        first_day, first_month, first_year = None, None, None\n",
    "        latest_day, latest_month, latest_year = None, None, None\n",
    "\n",
    "        chapter_time_divs = soup.find_all('div', class_='chapter-time')\n",
    "        if chapter_time_divs:\n",
    "            first_date_text = chapter_time_divs[0].get_text(strip=True)\n",
    "            latest_date_text = chapter_time_divs[-1].get_text(strip=True)\n",
    "        \n",
    "            first_day, first_month, first_year = parse_date(first_date_text)\n",
    "            latest_day, latest_month, latest_year = parse_date(latest_date_text)\n",
    "\n",
    "        return (url_id,title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "                trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, format_type,\n",
    "                first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting info for novel {novel_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_novel_info(info):\n",
    "    # Tách logic xử lý thông tin ra khỏi vòng lặp chính\n",
    "    (url_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "     author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "     trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, format_type,\n",
    "     first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom) = info\n",
    "    \n",
    "    return {\n",
    "        'ID': process_value(url_id),\n",
    "        'Tựa đề': process_value(title),\n",
    "        'Link hako': process_value(canonical_url),\n",
    "        'Phương thức sáng tác': process_value(method),\n",
    "        'Thể loại': process_value(genres),\n",
    "        'Manga': process_value(manga),\n",
    "        'Anime': process_value(anime),\n",
    "        'CD': process_value(cd),\n",
    "        'Ngôn ngữ gốc': process_value(origin),\n",
    "        'Link ảnh': process_value(image_link),\n",
    "        'Tác giả': process_value(author),\n",
    "        'Họa sĩ': process_value(artist),\n",
    "        'Loại hình': process_value(showtype),\n",
    "        'Tình trạng': process_value(state),\n",
    "        'Số like': convert_to_number(process_value(like, is_numeric=True)),\n",
    "        'Số từ': convert_to_number(process_value(nword, is_numeric=True)),\n",
    "        'Số lượt đánh giá': convert_to_number(process_value(rate, is_numeric=True)),\n",
    "        'Số lượt xem': convert_to_number(process_value(view, is_numeric=True)),\n",
    "        'Số lượt bình luận': convert_to_number(process_value(ncom, is_numeric=True)),\n",
    "        'Fname': process_value(fname),\n",
    "        'ID người dịch': process_value(id_o),\n",
    "        'Người dịch': process_value(trans),\n",
    "        'Link người dịch': process_value(id_o_l),\n",
    "        'ID nhóm dịch': process_value(id_t),\n",
    "        'Nhóm dịch': process_value(team),\n",
    "        'Link nhóm dịch': process_value(id_t_l),\n",
    "        'ID người tham gia': process_value(id_j),\n",
    "        'Người tham gia': process_value(atb),\n",
    "        'Link người tham gia': process_value(id_j_l),\n",
    "        'Số tập': process_value(nvol),\n",
    "        'Số chương': process_value(nchap),\n",
    "        'Hình thức': process_value(format_type),\n",
    "        'Ngày bắt đầu': process_value(first_day),\n",
    "        'Tháng bắt đầu': process_value(first_month),   \n",
    "        'Năm bắt đầu': process_value(first_year),\n",
    "        'Ngày cập nhật cuối': process_value(latest_day),\n",
    "        'Tháng cập nhật cuối': process_value(latest_month),\n",
    "        'Năm cập nhật cuối': process_value(latest_year)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_novel_batch(start_id, end_id, batch_size=100):\n",
    "    all_data = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Chia thành các batch nhỏ\n",
    "        batches = [(i, min(i + batch_size - 1, end_id)) \n",
    "                  for i in range(start_id, end_id + 1, batch_size)]\n",
    "        \n",
    "        futures = []\n",
    "        for batch_start, batch_end in batches:\n",
    "            future = executor.submit(process_batch, batch_start, batch_end)\n",
    "            futures.append(future)\n",
    "            \n",
    "        # Theo dõi tiến độ\n",
    "        total_batches = len(batches)\n",
    "        completed = 0\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            batch_data = future.result()\n",
    "            if batch_data:\n",
    "                all_data.extend(batch_data)\n",
    "            completed += 1\n",
    "            print(f\"Completed: {completed}/{total_batches} batches ({(completed/total_batches)*100:.2f}%)\")\n",
    "            \n",
    "            # Lưu checkpoint định kỳ\n",
    "            if completed % 10 == 0:  # Lưu sau mỗi 10 batch\n",
    "                save_checkpoint(all_data, f'checkpoint_{completed}.xlsx')\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def process_batch(start_id, end_id):\n",
    "    batch_data = []\n",
    "    for novel_id in range(start_id, end_id + 1):\n",
    "        try:\n",
    "            html_content = get_novel_html(novel_id)\n",
    "            if not html_content:\n",
    "                continue\n",
    "                \n",
    "            info = extract_info_from_html(novel_id, html_content)\n",
    "            if not info:\n",
    "                continue\n",
    "                \n",
    "            # Xử lý thông tin và thêm vào batch_data\n",
    "            processed_data = process_novel_info(info)\n",
    "            batch_data.append(processed_data)\n",
    "            \n",
    "            time.sleep(1)  # Giảm delay xuống\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing novel {novel_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "def save_checkpoint(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved checkpoint to {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 1/1 batches (100.00%)\n",
      "Completed! Saved 3 novels to hako_dataset_complete.csv\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng\n",
    "if __name__ == '__main__':\n",
    "    start_id = 1\n",
    "    end_id = 3\n",
    "    output_excel_path = 'hako_dataset_complete.csv'\n",
    "    \n",
    "    # Chạy với batch size 100\n",
    "    all_data = process_novel_batch(start_id, end_id, batch_size=100)\n",
    "    \n",
    "    # Lưu kết quả cuối cùng\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(output_excel_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Completed! Saved {len(all_data)} novels to {output_excel_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
